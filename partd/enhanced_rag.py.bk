"""
Enhanced RAG System with Web Search Fallback
Connects retrieval with generation and includes self-learning capabilities
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from typing import Dict, List, Optional
import logging
import json
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedRAG:
    """
    RAG system with:
    - Local vector database retrieval
    - Web search fallback
    - Self-learning from feedback (BONUS)
    """
    
    def __init__(self, retrieval_pipeline, llm_client, 
                 confidence_threshold: float = 0.5,
                 enable_web_search: bool = True,
                 feedback_storage: Optional[str] = None):
        """
        Initialize Enhanced RAG.
        
        Args:
            retrieval_pipeline: Milvus retrieval pipeline
            llm_client: Gemini LLM client
            confidence_threshold: Minimum score to trust local results
            enable_web_search: Enable web search fallback
            feedback_storage: Path to store feedback for self-learning (BONUS)
        """
        self.retrieval = retrieval_pipeline
        self.llm = llm_client
        self.confidence_threshold = confidence_threshold
        self.enable_web_search = enable_web_search
        
        # BONUS: Self-learning storage
        self.feedback_storage = Path(feedback_storage) if feedback_storage else Path("partd/feedback_storage")
        self.feedback_storage.mkdir(exist_ok=True, parents=True)
        
        # Initialize web search
        if self.enable_web_search:
            try:
                from duckduckgo_search import DDGS
                self.web_search_available = True
                logger.info("✓ Web search enabled")
            except ImportError:
                self.web_search_available = False
                logger.warning("⚠ Web search not available")
    
    def answer_question(self, question: str, top_k: int = 5, 
                       conversation_history: Optional[List[Dict]] = None) -> Dict:
        """
        Answer question using RAG with adaptive explanations.
        
        Args:
            question: User's question
            top_k: Number of chunks to retrieve
            conversation_history: Previous conversation for context
            
        Returns:
            Dict with answer, sources, and metadata
        """
        logger.info(f"Processing question: {question}")
        
        # Step 1: Local retrieval
        local_results = self.retrieval.hierarchical_retrieve(
            question,
            top_k=top_k,
            expand_context=True,
            rerank=True
        )
        
        has_good_results = self._evaluate_retrieval_quality(local_results)
        
        # Step 2: Web search if needed
        use_web_search = (
            not has_good_results and 
            self.enable_web_search and 
            self.web_search_available
        )
        
        web_results = []
        if use_web_search:
            logger.info("⚠ Using web search fallback")
            web_results = self._perform_web_search(question)
        
        # Step 3: Build context
        context = self._build_context(
            local_results.get('results', [])[:3],
            web_results
        )
        
        # Step 4: Check feedback history for improvements (BONUS)
        feedback_context = self._get_feedback_context(question)
        
        # Step 5: Generate adaptive answer
        answer = self._generate_adaptive_answer(
            question, 
            context, 
            conversation_history,
            feedback_context,
            has_good_results, 
            use_web_search
        )
        
        return {
            "question": question,
            "answer": answer,
            "sources": {
                "local": local_results.get('results', [])[:3],
                "web": web_results
            },
            "metadata": {
                "used_web_search": use_web_search,
                "num_local_sources": len(local_results.get('results', [])),
                "num_web_sources": len(web_results),
                "retrieval_confidence": self._calculate_confidence(local_results),
                "applied_feedback": len(feedback_context) > 0
            }
        }
    
    def _evaluate_retrieval_quality(self, results: Dict) -> bool:
        """Evaluate retrieval quality."""
        if not results.get('results'):
            return False
        
        top_result = results['results'][0]
        top_score = top_result.get('rerank_score', top_result.get('score', 0))
        
        logger.info(f"Top result score: {top_score:.3f}")
        
        return top_score >= self.confidence_threshold
    
    def _calculate_confidence(self, results: Dict) -> float:
        """Calculate confidence score."""
        if not results.get('results'):
            return 0.0
        
        top_result = results['results'][0]
        return float(top_result.get('rerank_score', top_result.get('score', 0)))
    
    def _perform_web_search(self, query: str, num_results: int = 3) -> List[Dict]:
        """Perform web search."""
        try:
            from duckduckgo_search import DDGS
            
            ddgs = DDGS()
            results = []
            
            search_results = ddgs.text(query, max_results=num_results)
            
            for result in search_results:
                results.append({
                    "title": result.get("title", ""),
                    "snippet": result.get("body", ""),
                    "url": result.get("href", ""),
                    "source": "web_search"
                })
            
            logger.info(f"✓ Web search: {len(results)} results")
            return results
        
        except Exception as e:
            logger.error(f"Web search error: {e}")
            return []
    
    def _build_context(self, local_results: List[Dict], web_results: List[Dict]) -> str:
        """Build context from sources."""
        context_parts = []
        
        if local_results:
            context_parts.append("=== Knowledge Base ===\n")
            for i, result in enumerate(local_results, 1):
                source = result.get('source_file', 'Unknown')
                score = result.get('rerank_score', result.get('score', 0))
                context_parts.append(f"[Source {i}: {source}] (Confidence: {score:.2f})")
                context_parts.append(result['text'][:1000])
                context_parts.append("")
        
        if web_results:
            context_parts.append("\n=== Recent Web Information ===\n")
            for i, result in enumerate(web_results, 1):
                context_parts.append(f"[Web {i}: {result['title']}]")
                context_parts.append(result['snippet'])
                context_parts.append("")
        
        return "\n".join(context_parts)
    
    def _get_feedback_context(self, question: str) -> List[Dict]:
        """
        BONUS: Get relevant feedback from self-learning storage.
        
        Args:
            question: Current question
            
        Returns:
            List of relevant feedback items
        """
        feedback_file = self.feedback_storage / "feedback_log.json"
        
        if not feedback_file.exists():
            return []
        
        try:
            with open(feedback_file, 'r') as f:
                all_feedback = json.load(f)
            
            # Find feedback for similar questions
            relevant = []
            question_lower = question.lower()
            
            for item in all_feedback:
                if any(keyword in question_lower for keyword in item.get('keywords', [])):
                    relevant.append(item)
            
            return relevant[:3]  # Top 3 most relevant
        
        except Exception as e:
            logger.error(f"Error loading feedback: {e}")
            return []
    
    def _generate_adaptive_answer(self, question: str, context: str,
                                  conversation_history: Optional[List[Dict]],
                                  feedback_context: List[Dict],
                                  has_local: bool, used_web: bool) -> str:
        """
        Generate adaptive answer with multi-step reasoning.
        
        Args:
            question: User's question
            context: Retrieved context
            conversation_history: Previous conversation
            feedback_context: Relevant feedback
            has_local: Has local results
            used_web: Used web search
            
        Returns:
            Generated answer
        """
        # Build conversation context
        conv_context = ""
        if conversation_history:
            conv_context = "\nRecent conversation:\n"
            for msg in conversation_history[-3:]:  # Last 3 messages
                role = msg.get('role', 'user')
                content = msg.get('content', '')
                conv_context += f"{role.title()}: {content[:200]}\n"
        
        # Build feedback context (BONUS)
        feedback_text = ""
        if feedback_context:
            feedback_text = "\nLearned improvements from feedback:\n"
            for fb in feedback_context:
                feedback_text += f"- {fb.get('improvement', '')}\n"
        
        system_prompt = f"""You are an expert in Environmental Sustainability and Awareness.

Your approach:
1. Provide clear, adaptive explanations based on context
2. Use multi-step reasoning to break down complex concepts
3. Promote environmental awareness and actionable insights
4. If using web information, mention "according to recent information"
5. Be educational and engaging

{feedback_text}"""
        
        user_prompt = f"""{conv_context}

Context Information:
{context}

Current Question: {question}

Provide a comprehensive, well-reasoned answer."""
        
        answer = self.llm.generate_with_context(system_prompt, user_prompt, max_tokens=1500)
        
        if used_web and not has_local:
            answer += "\n\n*Note: This includes recent web information as local knowledge was limited.*"
        
        return answer
    
    def record_feedback(self, question: str, answer: str, 
                       feedback_type: str, feedback_text: str,
                       rating: Optional[int] = None):
        """
        BONUS: Record user feedback for self-learning.
        
        Args:
            question: Original question
            answer: Generated answer
            feedback_type: Type of feedback (positive/negative/suggestion)
            feedback_text: Feedback content
            rating: Optional rating (1-5)
        """
        feedback_file = self.feedback_storage / "feedback_log.json"
        
        # Load existing feedback
        if feedback_file.exists():
            with open(feedback_file, 'r') as f:
                all_feedback = json.load(f)
        else:
            all_feedback = []
        
        # Extract keywords from question
        keywords = [word.lower() for word in question.split() if len(word) > 4]
        
        # Create feedback entry
        feedback_entry = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer[:500],  # Store preview
            "feedback_type": feedback_type,
            "feedback_text": feedback_text,
            "rating": rating,
            "keywords": keywords[:5],  # Top 5 keywords
            "improvement": self._extract_improvement(feedback_text)
        }
        
        all_feedback.append(feedback_entry)
        
        # Save feedback
        with open(feedback_file, 'w') as f:
            json.dump(all_feedback, f, indent=2)
        
        logger.info(f"✓ Feedback recorded: {feedback_type}")
        
        # Analyze and generate improvements
        if len(all_feedback) % 10 == 0:  # Every 10 feedback items
            self._analyze_feedback_patterns(all_feedback)
    
    def _extract_improvement(self, feedback_text: str) -> str:
        """Extract actionable improvement from feedback."""
        # Simple extraction - in production, use LLM
        if "more detail" in feedback_text.lower():
            return "Provide more detailed explanations"
        elif "simpler" in feedback_text.lower():
            return "Use simpler language"
        elif "example" in feedback_text.lower():
            return "Include more examples"
        else:
            return feedback_text[:100]
    
    def _analyze_feedback_patterns(self, feedback_list: List[Dict]):
        """
        BONUS: Analyze feedback patterns for learning.
        
        Args:
            feedback_list: All feedback items
        """
        logger.info("Analyzing feedback patterns...")
        
        analysis_file = self.feedback_storage / "feedback_analysis.json"
        
        # Count feedback types
        patterns = {
            "total": len(feedback_list),
            "positive": sum(1 for f in feedback_list if f['feedback_type'] == 'positive'),
            "negative": sum(1 for f in feedback_list if f['feedback_type'] == 'negative'),
            "avg_rating": sum(f.get('rating', 0) for f in feedback_list if f.get('rating')) / len([f for f in feedback_list if f.get('rating')]) if any(f.get('rating') for f in feedback_list) else 0,
            "common_improvements": []
        }
        
        # Find common improvements
        improvements = {}
        for f in feedback_list:
            imp = f.get('improvement', '')
            improvements[imp] = improvements.get(imp, 0) + 1
        
        patterns['common_improvements'] = sorted(improvements.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # Save analysis
        with open(analysis_file, 'w') as f:
            json.dump(patterns, f, indent=2)
        
        logger.info(f"✓ Feedback analysis saved: {len(feedback_list)} items analyzed")
